{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c16b236-fd9a-41cc-902b-bd5786db4f42",
   "metadata": {},
   "source": [
    "## 第十届 Sky Hackthon\n",
    "\n",
    "### 基于RAG技术创新构建智能对话机器人\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52eeff-644e-4e2b-8d55-e223c7958b5d",
   "metadata": {},
   "source": [
    "此次黑客松的目的是利用NVIDIA ai endpoint和NIM工具， 结合RAG技术，构建基于本地知识库的对话机器人\n",
    "\n",
    "* NVIDIA AI Endpoint介绍页面： https://python.langchain.com/v0.1/docs/integrations/chat/nvidia_ai_endpoints/\n",
    "* NVIDIA NIM页面： https://build.nvidia.com/explore/discover\n",
    "* NVIDIA DLI课程学习资料页面：https://www.nvidia.cn/training/online/\n",
    "------------------\n",
    "\n",
    "![](https://v.png.pub/imgs/2024/07/01/01454215ad77cf50.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19277306-7f08-408a-a6a1-3719e0a96795",
   "metadata": {},
   "source": [
    "### 环境确认\n",
    "首先您需要执行：\n",
    "\n",
    "`!pip list`\n",
    "\n",
    "来确认langchain-nvidia-ai-endpoints版本为0.0.11\n",
    "\n",
    "如果不是，请使用下面的命令重新安装：\n",
    "\n",
    "`!pip install langchain-nvidia-ai-endpoints==0.0.11`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e471477-acb8-4aeb-99de-fb4c7eed51e0",
   "metadata": {},
   "source": [
    "### 第一步导入所需要的工具库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fad9ee-52f9-41a2-a9ec-244924186c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b2f99-f66f-4ec2-8770-bfa904523488",
   "metadata": {},
   "source": [
    "### 第二步，设置API Key\n",
    "\n",
    "注意此处您需要在NVIDIA NIM页面中申请API Key： https://build.nvidia.com/explore/discover\n",
    "\n",
    "如下图所示：\n",
    "![](https://v.png.pub/imgs/2024/07/01/72fa333bb5bee55b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ec593-628d-4267-892b-7a1198fd4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "nvidia_api_key = \"Your API Key\"\n",
    "assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59c0c8-cf12-417c-9dc4-e12a8097708e",
   "metadata": {},
   "source": [
    "### 第三步， 设置对话模型\n",
    "\n",
    "在此处选择一个对话模型，并测试对话模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24a5cb-1ded-4d2a-88bb-7082ac51357b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model=\"ai-nemotron-4-340b-instruct\")\n",
    "result = llm.invoke(\"what is nemo\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11abfb-350f-4b13-a680-97dd72a053a9",
   "metadata": {},
   "source": [
    "### 第四步，设置Embedding模型\n",
    "\n",
    "在此处选择一个embedding模型，并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a3d68-d1bf-4d0f-9b97-fabdf2325a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")\n",
    "embedder.embed_query(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad998a5b-1b13-481b-ac39-eba17c885ca2",
   "metadata": {},
   "source": [
    "### 第五步，读取文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450b8b3-769b-452f-8850-ed117ed19bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# 在这里我们读入文本数据并将它们准备到 vectorstore 中\n",
    "ps = os.listdir(\"./zh_data/\")\n",
    "data = []\n",
    "sources = []\n",
    "docs_name = []\n",
    "for p in ps:\n",
    "    if p.endswith('.txt'):\n",
    "        path2file=\"./zh_data/\"+p\n",
    "        docs_name.append(path2file)\n",
    "        with open(path2file,encoding=\"utf-8\") as f:\n",
    "            lines=f.readlines()\n",
    "            for line in lines:\n",
    "                if len(line)>=1:\n",
    "                    data.append(line)\n",
    "                    sources.append(path2file)\n",
    "\n",
    "documents=[d for d in data if d != '\\n']\n",
    "len(data), len(documents), data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5108430-84df-4026-9768-90a8839311b6",
   "metadata": {},
   "source": [
    "### 第六步，创建vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ffa85f-4272-4d4e-b9dc-b2906a8156e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "import faiss\n",
    "# create my own uuid\n",
    "text_splitter = CharacterTextSplitter(chunk_size=400, separator=\" \")\n",
    "docs = []\n",
    "metadatas = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1dd9a7-db9c-4ef5-9338-ca3d9f5fdbe4",
   "metadata": {},
   "source": [
    "#### 下面的代码仅在您需要重构embed时需要重新运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df537e-7ac8-47cc-8d60-c835893af851",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(documents):\n",
    "    splits = text_splitter.split_text(d)\n",
    "    #print(len(splits))\n",
    "    docs.extend(splits)\n",
    "    metadatas.extend([{\"source\": sources[i]}] * len(splits))\n",
    "### 将创建好的embed存储到本地\n",
    "store = FAISS.from_texts(docs, embedder , metadatas=metadatas)\n",
    "store.save_local('./embed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e436b-b0c7-4bad-bbd3-15fbf41792fb",
   "metadata": {},
   "source": [
    "**注意：上面步骤中创建好的embed再下次重新运行流程时，不必重新执行，可以利用下面的代码直接从本地读取已经创建成功的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bb4e2-1e36-47e3-a38e-46e5fe60c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 从本地读取已经创建好的embed\n",
    "vecstores = [FAISS.load_local(folder_path=\"/home/nvidia/10th_hackathon/embed/\", embeddings=embedder)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3e21b-4559-4b72-b7ac-e9dcee2cf30b",
   "metadata": {},
   "source": [
    "* 设置default_FAISS() 函数，初始化空向量存储，通过convstore储存对话向量\n",
    "* 设置aggregate_vstores（）函数将有用的文档信息存储在 docstore变量中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcd039-c645-44da-b67b-c4106e72bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## 初始化一个空的 FAISS 索引并将其他索引合并到其中\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "if 'docstore' not in globals():\n",
    "    docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8b8be-cc55-40ce-aeed-5713cfecb4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model=\"ai-nemotron-4-340b-instruct\") | StrOutputParser()\n",
    "convstore = default_FAISS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5773f-8442-4d54-8e4f-8ace8c48de83",
   "metadata": {},
   "source": [
    "### 第七步，构建RAG 链\n",
    "\n",
    "* 自动对话存储：save_memory_and_get_output函数允许向我们的对话添加新条目\n",
    "\n",
    "\n",
    "* doc2str:将文本分块转换成上下文字符串格式输出。\n",
    "\n",
    "\n",
    "* prompt提示和结构。\n",
    "  \n",
    "* 构建retrieval_chain\n",
    "  \n",
    "* 设置对话生成函数chat_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f9429-8200-4953-9b4a-7d5b34ca4f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_names_string = \"\\n\"\n",
    "for doc_name in docs_name:\n",
    "    doc_names_string += doc_name+\"\\n\"\n",
    "    \n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_names_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.Reply must more than 100 words)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## 将较长的文档重新排序到输出文本的中心， RunnableLambda在链中运行无参自定义函数 ，长上下文重排序（LongContextReorder）\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "    | RPrint()\n",
    ")\n",
    "stream_chain = chat_prompt | llm\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ##首先根据输入的消息进行检索\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## 然后流式传输stream_chain的结果\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## 优化信息打印的格式\n",
    "        if not return_buffer:\n",
    "            line_buffer += token\n",
    "            if \"\\n\" in line_buffer:\n",
    "                line_buffer = \"\"\n",
    "            if ((len(line_buffer)>84 and token and token[0] == \" \") or len(line_buffer)>100):\n",
    "                line_buffer = \"\"\n",
    "                yield \"\\n\"\n",
    "                token = \"  \" + token.lstrip()\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ##最后将聊天内容保存到对话内存缓冲区中\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9578ae0-5b28-41fd-80ad-cde16e81cf7e",
   "metadata": {},
   "source": [
    "### 第八步，使用Gradio框架构建前端RAG机器人界面与您的 Gradio 聊天机器人互动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c4b184-e142-4260-91c4-7a51277d26af",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=False, show_api=False, server_port=5000, server_name=\"0.0.0.0\")\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c83fdde-fe40-45c5-911a-6457eb629992",
   "metadata": {},
   "source": [
    "### 自学课程推荐\n",
    "如果您在实验过程中想了解更多NIM+RAG的技术细节，也可以学习一下两门NVIDIA DLI课程：\n",
    "\n",
    "1. [Building RAG Agents with LLMs](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-15+V1)\n",
    "2. [使用 NIM 大规模部署 RAG 工作流](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-19+V1-ZH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa003b8-412e-49b6-8efb-92c197dc9423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
